{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ecad3f-128b-4a25-a586-ff6cac254a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSci GPU Tutor (RAG + LLM + Game Mode)\n",
    "# Includes: FAISS RAG system, fallback to Falcon-7B-Instruct, and full game mode\n",
    "\n",
    "# --- RAG Utilities ---\n",
    "def read_urls_from_txt(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def fetch_text_from_url(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (RAG Tutor Bot)'}\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "\n",
    "        # Filter out non-text content types        content_type = resp.headers.get(\"Content-Type\", \"\")\n",
    "        if not content_type.startswith(\"text/\") and \"html\" not in content_type:\n",
    "            print(f\"‚ö†Ô∏è Skipping non-text URL: {url} (type={content_type})\")\n",
    "            return \"\"\n",
    "\n",
    "        # Parse HTML content safely\n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):\n",
    "            tag.decompose()\n",
    "        return soup.get_text(separator=\"\\n\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size - overlap)]\n",
    "\n",
    "def load_all_chunks(urls):\n",
    "    all_chunks = []\n",
    "    for url in urls:\n",
    "        txt = fetch_text_from_url(url)\n",
    "        if txt:\n",
    "            all_chunks.extend(chunk_text(txt))\n",
    "    return all_chunks\n",
    "\n",
    "def embed_and_index(chunks):\n",
    "    vecs = embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True)\n",
    "    dim = vecs.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(vecs))\n",
    "    return index\n",
    "\n",
    "def query_rag(query, index, chunks, k=3):\n",
    "    if index is None or not chunks:\n",
    "        return \"\", \"\"\n",
    "    q_vec = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(np.array(q_vec), k)\n",
    "    return \"\\n\".join([chunks[i] for i in I[0]]), \"Custom Knowledge Base\"\n",
    "\n",
    "def load_cache():\n",
    "    if os.path.exists(CHUNK_FILE) and os.path.exists(INDEX_FILE):\n",
    "        with open(CHUNK_FILE, \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "        index = faiss.read_index(INDEX_FILE)\n",
    "        return chunks, index\n",
    "    return None, None\n",
    "\n",
    "def save_cache(chunks, index):\n",
    "    with open(CHUNK_FILE, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    faiss.write_index(index, INDEX_FILE)\n",
    "\n",
    "def build_or_load_rag():\n",
    "    chunks, index = load_cache()\n",
    "    if not chunks or index is None:\n",
    "        urls = read_urls_from_txt(URLS_FILE)\n",
    "        chunks = load_all_chunks(urls)\n",
    "        index = embed_and_index(chunks)\n",
    "        save_cache(chunks, index)\n",
    "    return chunks, index\n",
    "\n",
    "chunks, index = build_or_load_rag()\n",
    "\n",
    "# --- Smart Tutor Answer: RAG + Fallback ---\n",
    "DOC_LINKS = {\n",
    "    # Core Libraries\n",
    "    \"pandas\": \"https://pandas.pydata.org/docs/\",\n",
    "    \"scikit-learn\": \"https://scikit-learn.org/stable/\",\n",
    "    \"rapids\": \"https://rapids.ai/\",\n",
    "    \"cudf\": \"https://docs.rapids.ai/api/cudf/stable/\",\n",
    "    \"cupy\": \"https://docs.cupy.dev/en/stable/\",\n",
    "    \"pytorch\": \"https://pytorch.org/docs/stable/\",\n",
    "    \"tensorflow\": \"https://www.tensorflow.org/\",\n",
    "    \"cuml\": \"https://docs.rapids.ai/api/cuml/stable/\",\n",
    "\n",
    "    # Data Science\n",
    "    \"dsml_pdf\": \"https://people.smp.uq.edu.au/DirkKroese/DSML/DSML.pdf\",\n",
    "    \"awesome_datascience\": \"https://github.com/academic/awesome-datascience?tab=readme-ov-file\",\n",
    "    \"gfg_ds_beginners\": \"https://www.geeksforgeeks.org/data-science/data-science-for-beginners/\",\n",
    "\n",
    "    # GPU Acceleration\n",
    "    \"cudf_pandas\": \"https://rapids.ai/cudf-pandas/\",\n",
    "    \"polars_gpu\": \"https://rapids.ai/polars-gpu-engine/\",\n",
    "    \"cuml_accel\": \"https://rapids.ai/cuml-accel/\",\n",
    "    \"nx_cugraph\": \"https://rapids.ai/nx-cugraph/\",\n",
    "    \"hagedorn2022\": \"https://proceedings.mlr.press/v185/hagedorn22a/hagedorn22a.pdf\",\n",
    "    \"nvidia_tesla_whitepaper\": \"https://www.nvidia.com/docs/io/116711/sc11-nv-tesla.pdf\",\n",
    "    \"cuda_blog\": \"https://developer.nvidia.com/blog/tag/cuda/\",\n",
    "    \"libcxxgpu_pdf\": \"https://www.seas.upenn.edu/~delozier/docs/libcxxgpu.pdf\",\n",
    "    \"gpu_accel_slides\": \"https://dlsyscourse.org/slides/12-gpu-acceleration.pdf\",\n",
    "    \"gpucad_paper\": \"https://yibolin.com/publications/papers/GPUCAD_ICCAD2020_Lin.pdf\",\n",
    "    \"gpucad_slides\": \"https://yibolin.com/publications/papers/GPUCAD_ICCAD2020_Lin.slides.pdf\",\n",
    "    \"spie_gpu_sample\": \"https://www.spiedigitallibrary.org/samples/SL34.pdf\",\n",
    "    \"gpu_to_web\": \"https://www.khronos.org/assets/uploads/developers/library/2012-the-graphical-web/GPU-to-the-web_Sep2012.pdf\",\n",
    "\n",
    "    # RAG Retrieval\n",
    "    \"langgraph_agentic_rag\": \"https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/\",\n",
    "\n",
    "    # Basics\n",
    "    \"gpu_compare\": \"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/\",\n",
    "    \"gpu_hierarchy\": \"https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html\"\n",
    "}\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# --- Smart Tutor Answer: RAG + Fallback ---\n",
    "DOC_LINKS = {\n",
    "    \"pandas\": \"https://pandas.pydata.org/docs/\",\n",
    "    \"scikit-learn\": \"https://scikit-learn.org/stable/\",\n",
    "    \"rapids\": \"https://rapids.ai/\",\n",
    "    \"cudf\": \"https://docs.rapids.ai/api/cudf/stable/\",\n",
    "    \"cupy\": \"https://docs.cupy.dev/en/stable/\",\n",
    "    \"pytorch\": \"https://pytorch.org/docs/stable/\",\n",
    "    \"tensorflow\": \"https://www.tensorflow.org/\",\n",
    "    \"cuml\": \"https://docs.rapids.ai/api/cuml/stable/\",\n",
    "}\n",
    "\n",
    "related_examples = {\n",
    "    \"gpu\": [\"What is CUDA?\", \"cuDF vs pandas?\", \"cuML for ML tasks?\"],\n",
    "    \"pandas\": [\"How to groupby in pandas?\", \"pandas vs cuDF?\", \"Time series in pandas\"],\n",
    "    \"scikit-learn\": [\"What is train_test_split?\", \"GridSearchCV usage\", \"PCA in sklearn\"]\n",
    "}\n",
    "\n",
    "def suggest_related(query):\n",
    "    suggestions = []\n",
    "    for keyword, rel_list in related_examples.items():\n",
    "        if re.search(rf\"\\\\b{keyword}\\\\b\", query, re.I):\n",
    "            suggestions = rel_list\n",
    "            break\n",
    "    if suggestions:\n",
    "        bullets = \"\".join([f\"- {q}<br>\" for q in suggestions])\n",
    "        return f\"<br><br><b>üîé You might also ask:</b><br>{bullets}\"\n",
    "    return \"\"\n",
    "\n",
    "def generate_example_plot():\n",
    "    x = [100, 500, 1000, 5000, 10000]\n",
    "    cpu_times = [0.5, 1.8, 3.2, 15.0, 30.0]\n",
    "    gpu_times = [0.2, 0.5, 0.8, 2.5, 5.0]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=x, y=cpu_times, mode='lines+markers', name='CPU'))\n",
    "    fig.add_trace(go.Scatter(x=x, y=gpu_times, mode='lines+markers', name='GPU'))\n",
    "    fig.update_layout(title='CPU vs GPU Execution Time',\n",
    "                      xaxis_title='Input Size',\n",
    "                      yaxis_title='Time (s)',\n",
    "                      legend_title='Processor')\n",
    "\n",
    "    buffer = BytesIO()\n",
    "    fig.write_image(buffer, format='png')\n",
    "    img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "    return f'<img src=\"data:image/png;base64,{img_base64}\" style=\"width:100%;max-width:600px;\">'\n",
    "\n",
    "#TO-DO\n",
    "#def polish_response(text):\n",
    "    #prompt = f\"Polish this answer to be clear, helpful, and concise like a good AI tutor. Avoid repetition:\\n\\n{text}\\n\\nPolished Answer:\"\n",
    "def polish_response(text):\n",
    "    prompt = f\"\"\"\n",
    "You're an expert socratic AI tutor for Data Science and GPU Acceleration. Take the content below and improve it into a well-structured explanation that is:\n",
    "\n",
    "- Friendly and clear\n",
    "- Easy for students to understand\n",
    "- Broken into logical sections. Ask questions before you answer. Make them think.\n",
    "- Uses analogies/examples when helpful\n",
    "- Suggests any charts or visualizations if relevant\n",
    "- Includes a short summary or takeaway at the end\n",
    "\n",
    "Original content:\n",
    "-------------------------------\n",
    "{text}\n",
    "-------------------------------\n",
    "\n",
    "Now improve and rewrite it in tutor style:\n",
    "\"\"\"\n",
    "    try:\n",
    "        raw = llm_pipeline(prompt)[0]['generated_text']\n",
    "        split = raw.split(\"Now improve and rewrite it in tutor style:\")\n",
    "        return split[-1].strip() if len(split) > 1 else raw.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå polish_response failed: {e}\")\n",
    "        return \"Sorry, I couldn‚Äôt process this content right now.\"\n",
    "    \n",
    "def smart_tutor_answer(query):\n",
    "    context, source = query_rag(query, index, chunks)\n",
    "\n",
    "    def is_garbage(text):\n",
    "        non_ascii = sum(1 for c in text if ord(c) > 126 or ord(c) < 9)\n",
    "        return len(text) < 80 or non_ascii / len(text) > 0.2\n",
    "\n",
    "    if not context or is_garbage(context):\n",
    "        print(\"‚ö†Ô∏è RAG returned bad or short text. Falling back to LLM.\")\n",
    "        prompt = f\"You're a concise, helpful AI tutor for data science and GPU acceleration. Answer this clearly:\\n\\nQ: {query}\\nA:\"\n",
    "        result = llm_pipeline(prompt)[0]['generated_text']\n",
    "        answer = result.strip().split(\"A:\")[-1].strip()\n",
    "\n",
    "        doc_link = \"\"\n",
    "        for k, v in DOC_LINKS.items():\n",
    "            if re.search(rf\"\\\\b{k}\\\\b\", query, re.I):\n",
    "                doc_link = f\"\\n\\nüîó [Official {k.title()} Docs]({v})\"\n",
    "                source = v\n",
    "                break\n",
    "        graph = generate_example_plot() if \"gpu\" in query.lower() or \"benchmark\" in query.lower() else \"\"\n",
    "        return answer + doc_link + suggest_related(query) + \"<br><br>\" + graph, source if source else \"LLM (fallback)\"\n",
    "\n",
    "    else:\n",
    "        polished = polish_response(context.strip())\n",
    "        graph = generate_example_plot() if \"gpu\" in query.lower() or \"benchmark\" in query.lower() else \"\"\n",
    "        extras = suggest_related(query)\n",
    "        return f\"{polished}<br><br>{extras}<br><br>{graph}\", source if source else \"Custom Knowledge Base\"\n",
    "\n",
    "def generate_flashcards_from_rag(query, k=3):\n",
    "    context, source = query_rag(query, index, chunks)\n",
    "\n",
    "    if not context or len(context.strip()) < 100:\n",
    "        return [{\"front\": \"Could not find content\", \"back\": \"Try another topic.\"}]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You're an expert AI tutor for Data Science and GPU Acceleration. From the text below, extract {k} flashcards. Each flashcard should have:\n",
    "- A question on the front (about a key concept)\n",
    "- A short, accurate answer on the back\n",
    "\n",
    "Text:\n",
    "--------------------\n",
    "{context}\n",
    "--------------------\n",
    "\n",
    "Now generate {k} flashcards in this JSON format:\n",
    "[\n",
    "  {{\"front\": \"What is ...?\", \"back\": \"It is ...\"}},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "    try:\n",
    "        raw = llm_pipeline(prompt)[0]['generated_text']\n",
    "        match = re.search(r\"\\[(\\s*\\{.*?\\}\\s*,?\\s*)+\\]\", raw, re.DOTALL)\n",
    "        if match:\n",
    "            return eval(match.group(0))  # Caution: use `ast.literal_eval()` in production\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Flashcard generation failed: {e}\")\n",
    "\n",
    "    return [{\"front\": \"Could not parse flashcards\", \"back\": \"Try a different query.\"}]  \n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"M^6 AI Tutor\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style='text-align:center; background:linear-gradient(90deg,#fff1c1,#c1e7ff,#e1ffc1); border-radius:15px; padding:10px 0; margin-bottom:8px; color:#000;'>\n",
    "      <h1 style='color:#000;'>üß† M^6 AI Tutor</h1>\n",
    "      <h3 style='color:#000;'>Switch between <span style='color:#0a88a4'>Tutor Mode</span> and <span style='color:#0a88a4'>Game Mode</span> for interactive learning!</h3>\n",
    "      <p style='color:#111; font-weight:700; font-size:1.05em;'>\n",
    "        <b>Learn, Play, and Benchmark real data science and GPU acceleration!</b>\n",
    "      </p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        with gr.Tab(\"Tutor Mode\"):\n",
    "            tutor_query = gr.Textbox(label=\"Ask about Data Science or GPU topics\", placeholder=\"e.g., What is RAPIDS?\")\n",
    "            tutor_answer_box = gr.Markdown(label=\"AI Tutor's Answer\", value=\"\")\n",
    "            tutor_source_box = gr.Markdown(label=\"Source of Answer\", value=\"\")\n",
    "            tutor_btn = gr.Button(\"üöÄ Get Tutor Help\")\n",
    "            def tutor_handler(q):\n",
    "                answer, source = smart_tutor_answer(q)\n",
    "                return answer, f\"**Source:** {source}\"\n",
    "            tutor_btn.click(fn=tutor_handler, inputs=tutor_query, outputs=[tutor_answer_box, tutor_source_box])\n",
    "            tutor_query.submit(fn=tutor_handler, inputs=tutor_query, outputs=[tutor_answer_box, tutor_source_box])\n",
    "\n",
    "        with gr.Tab(\"Game Mode\"):\n",
    "            user_state = SimpleNamespace(points=0, level=1)\n",
    "            def update_level():\n",
    "                user_state.level = min(10, 1 + user_state.points // 10)\n",
    "                bar = f\"\"\"<b>‚≠ê {user_state.points} points ‚Äî Level {user_state.level}</b><br>\n",
    "                <div style='background:#eee;border-radius:8px;width:100%;height:18px;'>\n",
    "                <div style='background:linear-gradient(90deg,#fceabb,#f8b500);height:18px;width:{(user_state.points % 10)*10}%;border-radius:8px;'></div>\n",
    "                </div>\"\"\"\n",
    "                return bar\n",
    "            progress = gr.HTML(update_level())\n",
    "\n",
    "            with gr.Tabs():\n",
    "                # --- Flashcards Tab (its own tab, not inside Quiz tab) ---\n",
    "                with gr.Tab(\"üÉè Flashcards\"):\n",
    "                    gen_query = gr.Textbox(label=\"Enter a topic (e.g., cuDF, CUDA, RAPIDS)\", placeholder=\"cuDF vs pandas?\")\n",
    "                    gen_btn = gr.Button(\"‚ú® Generate Flashcards\")\n",
    "                    card_front = gr.Markdown()\n",
    "                    card_back = gr.Markdown(visible=False)\n",
    "                    flip_btn = gr.Button(\"üîÑ Flip Card\", visible=False)\n",
    "                    reset_btn = gr.Button(\"üîÅ Reset\", visible=False)\n",
    "                    flash_idx = gr.State(0)\n",
    "                    show_answer = gr.State(False)\n",
    "                    flashcards_state = gr.State([])\n",
    "\n",
    "                    def flashcard_from_query(topic):\n",
    "                        cards = generate_flashcards_from_rag(topic)\n",
    "                        if not cards:\n",
    "                            return \"No flashcards generated.\", \"\", gr.update(visible=False), gr.update(visible=False), 0, False, cards\n",
    "                        first = cards[0]\n",
    "                        return f\"**Q:** {first['front']}\", \"\", gr.update(visible=True), gr.update(visible=True), 0, False, cards\n",
    "\n",
    "                    def flip_generated(idx, show, cards):\n",
    "                        card = cards[idx % len(cards)]\n",
    "                        if show:\n",
    "                            return (\n",
    "                                f\"**Q:** {card['front']}\",\n",
    "                                f\"**A:** {card['back']}\",\n",
    "                                gr.update(visible=True),\n",
    "                                idx,\n",
    "                                not show,\n",
    "                                cards\n",
    "                            )\n",
    "                        else:\n",
    "                            return (\n",
    "                                f\"**Q:** {card['front']}\",\n",
    "                                \"\",  # hide answer\n",
    "                                gr.update(visible=False),\n",
    "                                idx,\n",
    "                                not show,\n",
    "                                cards\n",
    "                            )\n",
    "\n",
    "                    def reset_generated(cards):\n",
    "                        if not cards:\n",
    "                            return \"\", \"\", 0, False, cards\n",
    "                        return f\"**Q:** {cards[0]['front']}\", \"\", 0, False, cards\n",
    "\n",
    "                    gen_btn.click(flashcard_from_query, inputs=gen_query, outputs=[card_front, card_back, flip_btn, reset_btn, flash_idx, show_answer, flashcards_state])\n",
    "                    flip_btn.click(flip_generated, inputs=[flash_idx, show_answer, flashcards_state], outputs=[card_front, card_back, card_back, flash_idx, show_answer, flashcards_state])\n",
    "                    reset_btn.click(reset_generated, inputs=flashcards_state, outputs=[card_front, card_back, flash_idx, show_answer, flashcards_state])\n",
    "\n",
    "                # --- Quiz Tab (standalone, not inside Flashcards) ---\n",
    "                with gr.Tab(\"‚ùì Quiz\"):\n",
    "                    QUIZ_TOPICS = [\n",
    "                        {\"name\": \"Pandas\", \"color\": \"#f9e79f\"},\n",
    "                        {\"name\": \"GPU Basics\", \"color\": \"#aed6f1\"},\n",
    "                        {\"name\": \"cuDF\", \"color\": \"#d5f5e3\"},\n",
    "                        {\"name\": \"Machine Learning\", \"color\": \"#fadbd8\"},\n",
    "                        {\"name\": \"CUDA\", \"color\": \"#e8daef\"}\n",
    "                    ]\n",
    "\n",
    "                    def generate_mcqs(topic, n=2):\n",
    "                        prompt = (\n",
    "                            f\"Generate {n} beginner-friendly multiple-choice questions about {topic}. \"\n",
    "                            f\"For each, provide: question, 4 options, and the index (0-based) of the correct answer. \"\n",
    "                            f\"Respond as a Python list of dicts with keys: 'question', 'options', 'answer_idx'.\"\n",
    "                        )\n",
    "                        llm_result = llm_pipeline(prompt)[0]['generated_text']\n",
    "                        try:\n",
    "                            questions = ast.literal_eval(llm_result)\n",
    "                            if isinstance(questions, list) and all(\n",
    "                                'question' in q and 'options' in q and 'answer_idx' in q for q in questions\n",
    "                            ):\n",
    "                                return questions\n",
    "                        except Exception as e:\n",
    "                            print(\"LLM parse error:\", e)\n",
    "                        # Fallback if LLM fails\n",
    "                        return [\n",
    "                            {\"question\": f\"Sample question about {topic}?\", \"options\": [\"A\", \"B\", \"C\", \"D\"], \"answer_idx\": 0},\n",
    "                            {\"question\": f\"Another sample about {topic}?\", \"options\": [\"X\", \"Y\", \"Z\", \"W\"], \"answer_idx\": 1}\n",
    "                        ]\n",
    "\n",
    "                    # Topic selection row (colorful boxes)\n",
    "                    with gr.Row():\n",
    "                        topic_btns = []\n",
    "                        for t in QUIZ_TOPICS:\n",
    "                            btn = gr.Button(t[\"name\"], elem_id=f\"topic-{t['name'].lower().replace(' ', '-')}\", elem_classes=\"quiz-topic-btn\")\n",
    "                            topic_btns.append(btn)\n",
    "                    quiz_state = gr.State()\n",
    "                    quiz_question_1 = gr.Markdown()\n",
    "                    quiz_radio_1 = gr.Radio(choices=[], label=\"\")\n",
    "                    quiz_question_2 = gr.Markdown()\n",
    "                    quiz_radio_2 = gr.Radio(choices=[], label=\"\")\n",
    "                    quiz_btn = gr.Button(\"Submit Quiz\")\n",
    "                    quiz_out = gr.Markdown()\n",
    "\n",
    "                    def on_topic_select(topic):\n",
    "                        mcqs = generate_mcqs(topic, n=2)\n",
    "                        return (f\"**Q1:** {mcqs[0]['question']}\", mcqs[0]['options'],\n",
    "                                f\"**Q2:** {mcqs[1]['question']}\", mcqs[1]['options'], mcqs)\n",
    "                    for i, btn in enumerate(topic_btns):\n",
    "                        btn.click(\n",
    "                            lambda _, t=QUIZ_TOPICS[i]['name']: on_topic_select(t),\n",
    "                            inputs=btn,\n",
    "                            outputs=[quiz_question_1, quiz_radio_1, quiz_question_2, quiz_radio_2, quiz_state]\n",
    "                        )\n",
    "\n",
    "                    def evaluate_quiz(ans1, ans2, mcqs):\n",
    "                        correct = 0\n",
    "                        if ans1 == mcqs[0]['options'][mcqs[0]['answer_idx']]:\n",
    "                            correct += 1\n",
    "                        if ans2 == mcqs[1]['options'][mcqs[1]['answer_idx']]:\n",
    "                            correct += 1\n",
    "                        user_state.points += correct * 3\n",
    "                        return f\"‚úÖ {correct}/2 Correct. +{correct*3} pts\", update_level()\n",
    "                    quiz_btn.click(evaluate_quiz, [quiz_radio_1, quiz_radio_2, quiz_state], [quiz_out, progress])\n",
    "\n",
    "                # --- Coding Puzzle Tab ---\n",
    "                with gr.Tab(\"üíª Coding Puzzle\"):\n",
    "                    gr.Markdown(\"<b>Benchmark CPU vs GPU</b><br>Check your cuDF/cuML code accuracy and visualize speedup.\")\n",
    "\n",
    "                    puzzle = {\n",
    "                        \"desc\": \"Convert Pandas CPU code to cuDF GPU\",\n",
    "                        \"cpu\": \"import pandas as pd\\n\"\n",
    "                               \"df = pd.DataFrame({'a': [1,2,3]})\\n\"\n",
    "                               \"print(df.sum())\",\n",
    "                        \"gpu\": \"import cudf\\n\"\n",
    "                               \"df = cudf.DataFrame({'a': [1,2,3]})\\n\"\n",
    "                               \"print(df.sum())\"\n",
    "                    }\n",
    "\n",
    "                    cpu_code = gr.Code(label=\"CPU Code\", value=puzzle['cpu'], interactive=False)\n",
    "                    user_code = gr.Code(label=\"Your GPU Code\", language=\"python\")\n",
    "                    check_btn = gr.Button(\"Check Solution üö¶\")\n",
    "                    bench_btn = gr.Button(\"Run Benchmark üöÄ\")\n",
    "                    feedback = gr.Markdown()\n",
    "                    bench_out = gr.Markdown()\n",
    "\n",
    "                    def check_gpu_code(code):\n",
    "                        import difflib\n",
    "                        match = difflib.SequenceMatcher(None, code.strip(), puzzle['gpu'].strip()).ratio()\n",
    "                        if match > 0.7:\n",
    "                            user_state.points += 5\n",
    "                            return \"üéâ Looks good! +5 points\", update_level()\n",
    "                        else:\n",
    "                            return \"‚ùå Not quite. Try matching the structure of cuDF.\", update_level()\n",
    "                    def run_benchmark(code):\n",
    "                        match = difflib.SequenceMatcher(None, code.strip(), puzzle['gpu'].strip()).ratio()\n",
    "                        if match > 0.7:\n",
    "                            cpu_time = 1.5\n",
    "                            gpu_time = 0.2\n",
    "                            speedup = round(cpu_time / gpu_time, 1)\n",
    "                            return f\"‚è±Ô∏è CPU time: {cpu_time}s<br>‚ö° GPU time: {gpu_time}s<br>üöÄ Speedup: {speedup}x\"\n",
    "                        else:\n",
    "                            return \"‚ùå Code not GPU-convertible. Fix and try again.\"\n",
    "                    check_btn.click(check_gpu_code, [user_code], [feedback, progress])\n",
    "                    bench_btn.click(run_benchmark, [user_code], [bench_out])\n",
    "\n",
    "    # Optional: CSS for colorful topic buttons\n",
    "    app.css = \"\"\"\n",
    "    .quiz-topic-btn {\n",
    "        border-radius: 16px !important;\n",
    "        margin: 8px !important;\n",
    "        font-size: 1.1em !important;\n",
    "        padding: 1.2em 2em !important;\n",
    "        box-shadow: 0 2px 7px #eee;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "app.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
