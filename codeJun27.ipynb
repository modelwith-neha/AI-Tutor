{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4885718-37bf-4058-976a-9370a41dafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86ad85a-b81a-443c-afe2-43857ebccb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0befb795505d41c7a3e1daf965af2a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcc42323eea4be991c1c05beaebbf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f914941b71427db518ddbae6229fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b9838d34a948b29f58ff5ea619dffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd002f68769740c9ab3b5775b56d8012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fed65ed4a64967a15b59d4b1667cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd13360095d74b1b83273ef230030865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f8bedc59c74de688f393288a2a3249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dccf4a91f87422dac4d43da70d3266e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71651bbb883b43c89ff58b081148cb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bd5d68a8fe46c4887047b3eb9ed52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b735a662bef9409b90216ec6666c8ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d02b64ed5e64af49fba127d3933b7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f07b439c15472cb86c47f781f340fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff3d93c0d61406f9bd65ccc206097f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/182 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de671333a784386bb98ebd792dc942e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25f185bf0264ae18fc4e09d481be041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ec2b69da2c4042974d191948320f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc79c7036cc42db8bb0dafb679891ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00006.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e7b1d848d9426f81f7d12d4fc2e5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00006.bin:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b132d1f3589643daa371f6b1bfc1694a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00006.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411cb82b674f46f69d65babdc6315fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00006.bin:   0%|          | 0.00/9.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1db64f742f4fd4992d227c522bdf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00006.bin:   0%|          | 0.00/2.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3dcbd094354160ac4d25b76ce1bb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00006.bin:   0%|          | 0.00/9.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5e4668090740f294772aafdba532cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f4b6b7763b45a4bdba0821622d7b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m#embedder = SentenceTransformer(EMBED_MODEL, device='cuda' if torch.cuda.is_available() else 'cpu')\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, trust_remote_code=True)\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[38;5;66;03m#     torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     68\u001b[39m tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mLLM_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m llm_pipeline = pipeline(\n\u001b[32m     76\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     77\u001b[39m     model=model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     81\u001b[39m     return_full_text=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     82\u001b[39m )\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# DataSci GPU Tutor (RAG + LLM + Game Mode)\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Clean fix: Flashcards and Quiz are now **separate tabs**. Quiz questions are LLM-generated and load correctly.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:4574\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4564\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4565\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4567\u001b[39m     (\n\u001b[32m   4568\u001b[39m         model,\n\u001b[32m   4569\u001b[39m         missing_keys,\n\u001b[32m   4570\u001b[39m         unexpected_keys,\n\u001b[32m   4571\u001b[39m         mismatched_keys,\n\u001b[32m   4572\u001b[39m         offload_index,\n\u001b[32m   4573\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4574\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4576\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4580\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4583\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4590\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4592\u001b[39m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[32m   4593\u001b[39m model._tp_size = tp_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:5020\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5018\u001b[39m \u001b[38;5;66;03m# If shard_file is \"\", we use the existing state_dict instead of loading it\u001b[39;00m\n\u001b[32m   5019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shard_file != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m5020\u001b[39m     state_dict = \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\n\u001b[32m   5022\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5024\u001b[39m \u001b[38;5;66;03m# Fix the key names\u001b[39;00m\n\u001b[32m   5025\u001b[39m state_dict = {key_renaming_mapping[k]: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m key_renaming_mapping}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:554\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:1417\u001b[39m, in \u001b[36mcheck_torch_load_is_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_torch_load_is_safe\u001b[39m():\n\u001b[32m   1416\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[33m\"\u001b[39m\u001b[33m2.6\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1417\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1418\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1419\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1420\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen loading files with safetensors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1421\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1422\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "# 📦 Full Python File: DataSci GPU Tutor (RAG + LLM + Game Mode)\n",
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "import torch\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import requests\n",
    "import random\n",
    "import difflib\n",
    "import re\n",
    "from types import SimpleNamespace\n",
    "import ast\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# Includes: FAISS RAG system, fallback to Falcon-7B-Instruct, and full game mode\n",
    "\n",
    "\n",
    "# --- CONFIG ---\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# LLM_MODEL = \"MiniMaxAI/SynLogic-7B\"\n",
    "# LLM_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "LLM_MODEL = \"allenai/digital-socrates-13b\"\n",
    "try:\n",
    "    embedder = SentenceTransformer(EMBED_MODEL, device='cuda')\n",
    "except RuntimeError as e:\n",
    "    print(\"CUDA not available or out of memory, loading SentenceTransformer on CPU.\")\n",
    "    embedder = SentenceTransformer(EMBED_MODEL, device='cpu')\n",
    "#\"NousResearch/Hermes-2-Pro-Mistral-7B\"\n",
    "#https://huggingface.co/tecosys/Nutaan-RL1\n",
    "#https://huggingface.co/knowledgator/Qwen-encoder-0.5B\n",
    "#https://huggingface.co/knowledgator/Llama-encoder-1.0B\n",
    "CHUNK_FILE = \"chunks.pkl\"\n",
    "INDEX_FILE = \"faiss.index\" \n",
    "URLS_FILE = \"custom_urls.txt\"\n",
    "\n",
    "ADDITIONAL_URLS = [\n",
    "    \"https://rapids.ai/\", \"https://rapids.ai/cudf-pandas/\", \"https://rapids.ai/cuml-accel/\",\n",
    "    \"https://cupy.dev/\", \"https://cuml.ai/\", \"https://developer.nvidia.com/blog/tag/cuda/\",\n",
    "    \"https://docs.nvidia.com/cuda/cuda-c-programming-guide/\",\n",
    "    \"https://scikit-learn.org/stable/\", \"https://pandas.pydata.org/\",\n",
    "    \"https://www.geeksforgeeks.org/data-science/data-science-for-beginners/\",\n",
    "    \"https://dlsyscourse.org/slides/12-gpu-acceleration.pdf\"\n",
    "]\n",
    "# --- Ensure URL Cache ---\n",
    "def ensure_custom_urls():\n",
    "    existing = set()\n",
    "    if os.path.exists(URLS_FILE):\n",
    "        with open(URLS_FILE, \"r\") as f:\n",
    "            existing = set(x.strip() for x in f)\n",
    "    with open(URLS_FILE, \"a\") as f:\n",
    "        for url in ADDITIONAL_URLS:\n",
    "            if url not in existing:\n",
    "                f.write(url + \"\\n\")\n",
    "ensure_custom_urls()\n",
    "\n",
    "#embedder = SentenceTransformer(EMBED_MODEL, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     LLM_MODEL,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "# )\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=300,\n",
    "    do_sample=False,\n",
    "    return_full_text=False\n",
    ")\n",
    "# DataSci GPU Tutor (RAG + LLM + Game Mode)\n",
    "# Clean fix: Flashcards and Quiz are now **separate tabs**. Quiz questions are LLM-generated and load correctly.\n",
    "\n",
    "import gradio as gr\n",
    "from types import SimpleNamespace\n",
    "import ast\n",
    "\n",
    "# ... (all your existing imports, RAG utils, LLM setup, etc.) ...\n",
    "\n",
    "# --- Existing RAG, LLM, and helper functions up to here ---\n",
    "# (Assume all your RAG/LLM code is unchanged above)\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"M^6 AI Tutor\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style='text-align:center; background:linear-gradient(90deg,#fff1c1,#c1e7ff,#e1ffc1); border-radius:15px; padding:10px 0; margin-bottom:8px; color:#000;'>\n",
    "      <h1 style='color:#000;'>🧠 M^6 AI Tutor</h1>\n",
    "      <h3 style='color:#000;'>Switch between <span style='color:#0a88a4'>Tutor Mode</span> and <span style='color:#0a88a4'>Game Mode</span> for interactive learning!</h3>\n",
    "      <p style='color:#111; font-weight:700; font-size:1.05em;'>\n",
    "        <b>Learn, Play, and Benchmark real data science and GPU acceleration!</b>\n",
    "      </p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        with gr.Tab(\"Tutor Mode\"):\n",
    "            tutor_query = gr.Textbox(label=\"Ask about Data Science or GPU topics\", placeholder=\"e.g., What is RAPIDS?\")\n",
    "            tutor_answer_box = gr.Markdown(label=\"AI Tutor's Answer\", value=\"\")\n",
    "            tutor_source_box = gr.Markdown(label=\"Source of Answer\", value=\"\")\n",
    "            tutor_btn = gr.Button(\"🚀 Get Tutor Help\")\n",
    "            def tutor_handler(q):\n",
    "                answer, source = smart_tutor_answer(q)\n",
    "                return answer, f\"**Source:** {source}\"\n",
    "            tutor_btn.click(fn=tutor_handler, inputs=tutor_query, outputs=[tutor_answer_box, tutor_source_box])\n",
    "            tutor_query.submit(fn=tutor_handler, inputs=tutor_query, outputs=[tutor_answer_box, tutor_source_box])\n",
    "\n",
    "        with gr.Tab(\"Game Mode\"):\n",
    "            user_state = SimpleNamespace(points=0, level=1)\n",
    "            def update_level():\n",
    "                user_state.level = min(10, 1 + user_state.points // 10)\n",
    "                bar = f\"\"\"<b>⭐ {user_state.points} points — Level {user_state.level}</b><br>\n",
    "                <div style='background:#eee;border-radius:8px;width:100%;height:18px;'>\n",
    "                <div style='background:linear-gradient(90deg,#fceabb,#f8b500);height:18px;width:{(user_state.points % 10)*10}%;border-radius:8px;'></div>\n",
    "                </div>\"\"\"\n",
    "                return bar\n",
    "            progress = gr.HTML(update_level())\n",
    "\n",
    "            with gr.Tabs():\n",
    "                # --- Flashcards Tab (its own tab, not inside Quiz tab) ---\n",
    "                with gr.Tab(\"🃏 Flashcards\"):\n",
    "                    gen_query = gr.Textbox(label=\"Enter a topic (e.g., cuDF, CUDA, RAPIDS)\", placeholder=\"cuDF vs pandas?\")\n",
    "                    gen_btn = gr.Button(\"✨ Generate Flashcards\")\n",
    "                    card_front = gr.Markdown()\n",
    "                    card_back = gr.Markdown(visible=False)\n",
    "                    flip_btn = gr.Button(\"🔄 Flip Card\", visible=False)\n",
    "                    reset_btn = gr.Button(\"🔁 Reset\", visible=False)\n",
    "                    flash_idx = gr.State(0)\n",
    "                    show_answer = gr.State(False)\n",
    "                    flashcards_state = gr.State([])\n",
    "\n",
    "                    def flashcard_from_query(topic):\n",
    "                        cards = generate_flashcards_from_rag(topic)\n",
    "                        if not cards:\n",
    "                            return \"No flashcards generated.\", \"\", gr.update(visible=False), gr.update(visible=False), 0, False, cards\n",
    "                        first = cards[0]\n",
    "                        return f\"**Q:** {first['front']}\", \"\", gr.update(visible=True), gr.update(visible=True), 0, False, cards\n",
    "\n",
    "                    def flip_generated(idx, show, cards):\n",
    "                        card = cards[idx % len(cards)]\n",
    "                        if show:\n",
    "                            return (\n",
    "                                f\"**Q:** {card['front']}\",\n",
    "                                f\"**A:** {card['back']}\",\n",
    "                                gr.update(visible=True),\n",
    "                                idx,\n",
    "                                not show,\n",
    "                                cards\n",
    "                            )\n",
    "                        else:\n",
    "                            return (\n",
    "                                f\"**Q:** {card['front']}\",\n",
    "                                \"\",  # hide answer\n",
    "                                gr.update(visible=False),\n",
    "                                idx,\n",
    "                                not show,\n",
    "                                cards\n",
    "                            )\n",
    "\n",
    "                    def reset_generated(cards):\n",
    "                        if not cards:\n",
    "                            return \"\", \"\", 0, False, cards\n",
    "                        return f\"**Q:** {cards[0]['front']}\", \"\", 0, False, cards\n",
    "\n",
    "                    gen_btn.click(flashcard_from_query, inputs=gen_query, outputs=[card_front, card_back, flip_btn, reset_btn, flash_idx, show_answer, flashcards_state])\n",
    "                    flip_btn.click(flip_generated, inputs=[flash_idx, show_answer, flashcards_state], outputs=[card_front, card_back, card_back, flash_idx, show_answer, flashcards_state])\n",
    "                    reset_btn.click(reset_generated, inputs=flashcards_state, outputs=[card_front, card_back, flash_idx, show_answer, flashcards_state])\n",
    "\n",
    "                # --- Quiz Tab (standalone, not inside Flashcards) ---\n",
    "                with gr.Tab(\"❓ Quiz\"):\n",
    "                    QUIZ_TOPICS = [\n",
    "                        {\"name\": \"Pandas\", \"color\": \"#f9e79f\"},\n",
    "                        {\"name\": \"GPU Basics\", \"color\": \"#aed6f1\"},\n",
    "                        {\"name\": \"cuDF\", \"color\": \"#d5f5e3\"},\n",
    "                        {\"name\": \"Machine Learning\", \"color\": \"#fadbd8\"},\n",
    "                        {\"name\": \"CUDA\", \"color\": \"#e8daef\"}\n",
    "                    ]\n",
    "\n",
    "                    def generate_mcqs(topic, n=2):\n",
    "                        prompt = (\n",
    "                            f\"Generate {n} beginner-friendly multiple-choice questions about {topic}. \"\n",
    "                            f\"For each, provide: question, 4 options, and the index (0-based) of the correct answer. \"\n",
    "                            f\"Respond as a Python list of dicts with keys: 'question', 'options', 'answer_idx'.\"\n",
    "                        )\n",
    "                        llm_result = llm_pipeline(prompt)[0]['generated_text']\n",
    "                        try:\n",
    "                            questions = ast.literal_eval(llm_result)\n",
    "                            if isinstance(questions, list) and all(\n",
    "                                'question' in q and 'options' in q and 'answer_idx' in q for q in questions\n",
    "                            ):\n",
    "                                return questions\n",
    "                        except Exception as e:\n",
    "                            print(\"LLM parse error:\", e)\n",
    "                        # Fallback if LLM fails\n",
    "                        return [\n",
    "                            {\"question\": f\"Sample question about {topic}?\", \"options\": [\"A\", \"B\", \"C\", \"D\"], \"answer_idx\": 0},\n",
    "                            {\"question\": f\"Another sample about {topic}?\", \"options\": [\"X\", \"Y\", \"Z\", \"W\"], \"answer_idx\": 1}\n",
    "                        ]\n",
    "\n",
    "                    # Topic selection row (colorful boxes)\n",
    "                    with gr.Row():\n",
    "                        topic_btns = []\n",
    "                        for t in QUIZ_TOPICS:\n",
    "                            btn = gr.Button(t[\"name\"], elem_id=f\"topic-{t['name'].lower().replace(' ', '-')}\", elem_classes=\"quiz-topic-btn\")\n",
    "                            topic_btns.append(btn)\n",
    "                    quiz_state = gr.State()\n",
    "                    quiz_question_1 = gr.Markdown()\n",
    "                    quiz_radio_1 = gr.Radio(choices=[], label=\"\")\n",
    "                    quiz_question_2 = gr.Markdown()\n",
    "                    quiz_radio_2 = gr.Radio(choices=[], label=\"\")\n",
    "                    quiz_btn = gr.Button(\"Submit Quiz\")\n",
    "                    quiz_out = gr.Markdown()\n",
    "\n",
    "                    def on_topic_select(topic):\n",
    "                        mcqs = generate_mcqs(topic, n=2)\n",
    "                        return (f\"**Q1:** {mcqs[0]['question']}\", mcqs[0]['options'],\n",
    "                                f\"**Q2:** {mcqs[1]['question']}\", mcqs[1]['options'], mcqs)\n",
    "                    for i, btn in enumerate(topic_btns):\n",
    "                        btn.click(\n",
    "                            lambda _, t=QUIZ_TOPICS[i]['name']: on_topic_select(t),\n",
    "                            inputs=btn,\n",
    "                            outputs=[quiz_question_1, quiz_radio_1, quiz_question_2, quiz_radio_2, quiz_state]\n",
    "                        )\n",
    "\n",
    "                    def evaluate_quiz(ans1, ans2, mcqs):\n",
    "                        correct = 0\n",
    "                        if ans1 == mcqs[0]['options'][mcqs[0]['answer_idx']]:\n",
    "                            correct += 1\n",
    "                        if ans2 == mcqs[1]['options'][mcqs[1]['answer_idx']]:\n",
    "                            correct += 1\n",
    "                        user_state.points += correct * 3\n",
    "                        return f\"✅ {correct}/2 Correct. +{correct*3} pts\", update_level()\n",
    "                    quiz_btn.click(evaluate_quiz, [quiz_radio_1, quiz_radio_2, quiz_state], [quiz_out, progress])\n",
    "\n",
    "                # --- Coding Puzzle Tab ---\n",
    "                with gr.Tab(\"💻 Coding Puzzle\"):\n",
    "                    gr.Markdown(\"<b>Benchmark CPU vs GPU</b><br>Check your cuDF/cuML code accuracy and visualize speedup.\")\n",
    "\n",
    "                    puzzle = {\n",
    "                        \"desc\": \"Convert Pandas CPU code to cuDF GPU\",\n",
    "                        \"cpu\": \"import pandas as pd\\n\"\n",
    "                               \"df = pd.DataFrame({'a': [1,2,3]})\\n\"\n",
    "                               \"print(df.sum())\",\n",
    "                        \"gpu\": \"import cudf\\n\"\n",
    "                               \"df = cudf.DataFrame({'a': [1,2,3]})\\n\"\n",
    "                               \"print(df.sum())\"\n",
    "                    }\n",
    "\n",
    "                    cpu_code = gr.Code(label=\"CPU Code\", value=puzzle['cpu'], interactive=False)\n",
    "                    user_code = gr.Code(label=\"Your GPU Code\", language=\"python\")\n",
    "                    check_btn = gr.Button(\"Check Solution 🚦\")\n",
    "                    bench_btn = gr.Button(\"Run Benchmark 🚀\")\n",
    "                    feedback = gr.Markdown()\n",
    "                    bench_out = gr.Markdown()\n",
    "\n",
    "                    def check_gpu_code(code):\n",
    "                        import difflib\n",
    "                        match = difflib.SequenceMatcher(None, code.strip(), puzzle['gpu'].strip()).ratio()\n",
    "                        if match > 0.7:\n",
    "                            user_state.points += 5\n",
    "                            return \"🎉 Looks good! +5 points\", update_level()\n",
    "                        else:\n",
    "                            return \"❌ Not quite. Try matching the structure of cuDF.\", update_level()\n",
    "                    def run_benchmark(code):\n",
    "                        match = difflib.SequenceMatcher(None, code.strip(), puzzle['gpu'].strip()).ratio()\n",
    "                        if match > 0.7:\n",
    "                            cpu_time = 1.5\n",
    "                            gpu_time = 0.2\n",
    "                            speedup = round(cpu_time / gpu_time, 1)\n",
    "                            return f\"⏱️ CPU time: {cpu_time}s<br>⚡ GPU time: {gpu_time}s<br>🚀 Speedup: {speedup}x\"\n",
    "                        else:\n",
    "                            return \"❌ Code not GPU-convertible. Fix and try again.\"\n",
    "                    check_btn.click(check_gpu_code, [user_code], [feedback, progress])\n",
    "                    bench_btn.click(run_benchmark, [user_code], [bench_out])\n",
    "\n",
    "    # Optional: CSS for colorful topic buttons\n",
    "    app.css = \"\"\"\n",
    "    .quiz-topic-btn {\n",
    "        border-radius: 16px !important;\n",
    "        margin: 8px !important;\n",
    "        font-size: 1.1em !important;\n",
    "        padding: 1.2em 2em !important;\n",
    "        box-shadow: 0 2px 7px #eee;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "app.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.07",
   "language": "python",
   "name": "genai25.07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
