{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293446c2-2d1b-476a-a482-bddec84185f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSci GPU Tutor (RAG + LLM + Game Mode)\n",
    "# Includes: FAISS RAG system, fallback to Falcon-7B-Instruct, and full game mode\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "import torch\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import requests\n",
    "import random\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Embedding model for RAG\n",
    "LLM_MODEL = \"tiiuae/falcon-7b-instruct\"                 # LLM for fallback and polish\n",
    "CHUNK_FILE = \"chunks.pkl\"\n",
    "INDEX_FILE = \"faiss.index\"\n",
    "URLS_FILE = \"custom_urls.txt\"\n",
    "\n",
    "# List of relevant URLs for RAG (Data Science, GPU, etc.)\n",
    "ADDITIONAL_URLS = [\n",
    "    \"https://rapids.ai/\", \"https://rapids.ai/cudf-pandas/\", \"https://rapids.ai/cuml-accel/\",\n",
    "    \"https://cupy.dev/\", \"https://cuml.ai/\", \"https://developer.nvidia.com/blog/tag/cuda/\",\n",
    "    \"https://docs.nvidia.com/cuda/cuda-c-programming-guide/\",\n",
    "    \"https://scikit-learn.org/stable/\", \"https://pandas.pydata.org/\",\n",
    "    \"https://www.geeksforgeeks.org/data-science/data-science-for-beginners/\",\n",
    "    \"https://dlsyscourse.org/slides/12-gpu-acceleration.pdf\"\n",
    "]\n",
    "\n",
    "# --- Ensure URLs are in file ---\n",
    "def ensure_custom_urls():\n",
    "    \"\"\"Make sure all ADDITIONAL_URLS are present in the URL file (no duplicates).\"\"\"\n",
    "    existing = set()\n",
    "    if os.path.exists(URLS_FILE):\n",
    "        with open(URLS_FILE, \"r\") as f:\n",
    "            existing = set(x.strip() for x in f)\n",
    "    with open(URLS_FILE, \"a\") as f:\n",
    "        for url in ADDITIONAL_URLS:\n",
    "            if url not in existing:\n",
    "                f.write(url + \"\\n\")\n",
    "ensure_custom_urls()\n",
    "\n",
    "# --- Load Models ---\n",
    "# SentenceTransformer for embeddings (used in RAG)\n",
    "embedder = SentenceTransformer(EMBED_MODEL, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Falcon LLM for fallback and polish\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
    "\n",
    "# --- RAG Utilities ---\n",
    "def read_urls_from_txt(path):\n",
    "    \"\"\"Read URLs (1 per line) from file.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def fetch_text_from_url(url):\n",
    "    \"\"\"Scrape and clean text from a URL (HTML/text only, skip other content).\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (RAG Tutor Bot)'}\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "        # Skip non-text URLs\n",
    "        content_type = resp.headers.get(\"Content-Type\", \"\")\n",
    "        if not content_type.startswith(\"text/\") and \"html\" not in content_type:\n",
    "            print(f\"‚ö†Ô∏è Skipping non-text URL: {url} (type={content_type})\")\n",
    "            return \"\"\n",
    "        # Clean HTML\n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):\n",
    "            tag.decompose()\n",
    "        return soup.get_text(separator=\"\\n\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"Chunk text into overlapping blocks for embedding.\"\"\"\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size - overlap)]\n",
    "\n",
    "def load_all_chunks(urls):\n",
    "    \"\"\"Scrape, chunk, and collect all URL text for RAG.\"\"\"\n",
    "    all_chunks = []\n",
    "    for url in urls:\n",
    "        txt = fetch_text_from_url(url)\n",
    "        if txt:\n",
    "            all_chunks.extend(chunk_text(txt))\n",
    "    return all_chunks\n",
    "\n",
    "def embed_and_index(chunks):\n",
    "    \"\"\"Embed all text chunks and build a FAISS index.\"\"\"\n",
    "    vecs = embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True)\n",
    "    dim = vecs.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(vecs))\n",
    "    return index\n",
    "\n",
    "def query_rag(query, index, chunks, k=3):\n",
    "    \"\"\"Query FAISS for top-k relevant chunks for the question.\"\"\"\n",
    "    if index is None or not chunks:\n",
    "        return \"\", \"\"\n",
    "    q_vec = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(np.array(q_vec), k)\n",
    "    return \"\\n\".join([chunks[i] for i in I[0]]), \"Custom Knowledge Base\"\n",
    "\n",
    "def load_cache():\n",
    "    \"\"\"Load prebuilt FAISS index and text chunks if present.\"\"\"\n",
    "    if os.path.exists(CHUNK_FILE) and os.path.exists(INDEX_FILE):\n",
    "        with open(CHUNK_FILE, \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "        index = faiss.read_index(INDEX_FILE)\n",
    "        return chunks, index\n",
    "    return None, None\n",
    "\n",
    "def save_cache(chunks, index):\n",
    "    \"\"\"Store text chunks and FAISS index to disk.\"\"\"\n",
    "    with open(CHUNK_FILE, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    faiss.write_index(index, INDEX_FILE)\n",
    "\n",
    "def build_or_load_rag():\n",
    "    \"\"\"Build or load the RAG system (chunks + FAISS index).\"\"\"\n",
    "    chunks, index = load_cache()\n",
    "    if not chunks or index is None:\n",
    "        urls = read_urls_from_txt(URLS_FILE)\n",
    "        chunks = load_all_chunks(urls)\n",
    "        index = embed_and_index(chunks)\n",
    "        save_cache(chunks, index)\n",
    "    return chunks, index\n",
    "\n",
    "# Build/load the index for RAG\n",
    "chunks, index = build_or_load_rag()\n",
    "\n",
    "# --- Smart Tutor Answer: RAG + Fallback ---\n",
    "DOC_LINKS = {\n",
    "    # Core Libraries/docs\n",
    "    \"pandas\": \"https://pandas.pydata.org/docs/\",\n",
    "    \"scikit-learn\": \"https://scikit-learn.org/stable/\",\n",
    "    \"rapids\": \"https://rapids.ai/\",\n",
    "    \"cudf\": \"https://docs.rapids.ai/api/cudf/stable/\",\n",
    "    \"cupy\": \"https://docs.cupy.dev/en/stable/\",\n",
    "    \"pytorch\": \"https://pytorch.org/docs/stable/\",\n",
    "    \"tensorflow\": \"https://www.tensorflow.org/\",\n",
    "    \"cuml\": \"https://docs.rapids.ai/api/cuml/stable/\",\n",
    "}\n",
    "\n",
    "# Related example queries for user suggestions\n",
    "related_examples = {\n",
    "    \"gpu\": [\"What is CUDA?\", \"cuDF vs pandas?\", \"cuML for ML tasks?\"],\n",
    "    \"pandas\": [\"How to groupby in pandas?\", \"pandas vs cuDF?\", \"Time series in pandas\"],\n",
    "    \"scikit-learn\": [\"What is train_test_split?\", \"GridSearchCV usage\", \"PCA in sklearn\"]\n",
    "}\n",
    "\n",
    "def suggest_related(query):\n",
    "    \"\"\"Suggest related questions for the user.\"\"\"\n",
    "    suggestions = []\n",
    "    for keyword, rel_list in related_examples.items():\n",
    "        if re.search(rf\"\\b{keyword}\\b\", query, re.I):\n",
    "            suggestions = rel_list\n",
    "            break\n",
    "    if suggestions:\n",
    "        bullets = \"\".join([f\"- {q}<br>\" for q in suggestions])\n",
    "        return f\"<br><br><b>üîé You might also ask:</b><br>{bullets}\"\n",
    "    return \"\"\n",
    "\n",
    "# --- Tutor Mode: AI Response Logic ---\n",
    "def smart_tutor_answer(query):\n",
    "    \"\"\"Answer a user query: try RAG, fallback to LLM, show source, suggest related.\"\"\"\n",
    "    context, source = query_rag(query, index, chunks)\n",
    "    def is_garbage(text):\n",
    "        non_ascii = sum(1 for c in text if ord(c) > 126 or ord(c) < 9)\n",
    "        return len(text) < 80 or non_ascii / len(text) > 0.2\n",
    "\n",
    "    # Use LLM if RAG is short or low quality\n",
    "    if not context or is_garbage(context):\n",
    "        print(\"‚ö†Ô∏è RAG returned bad or short text. Falling back to LLM.\")\n",
    "        prompt = f\"You're a concise, helpful AI tutor for data science and GPU acceleration. Answer this clearly:\\n\\nQ: {query}\\nA:\"\n",
    "        result = llm_pipeline(prompt)[0]['generated_text']\n",
    "        answer = result.strip().split(\"A:\")[-1].strip()\n",
    "        doc_link = \"\"\n",
    "        for k, v in DOC_LINKS.items():\n",
    "            if re.search(rf\"\\b{k}\\b\", query, re.I):\n",
    "                doc_link = f\"\\n\\nüîó [Official {k.title()} Docs]({v})\"\n",
    "                source = v\n",
    "                break\n",
    "        return answer + doc_link + suggest_related(query), source if source else \"LLM (fallback)\"\n",
    "    else:\n",
    "        # \"Polish\" the RAG answer using the LLM for clarity\n",
    "        prompt = f\"Polish this answer to be clear and helpful:\\n\\n{context}\\n\\nA:\"\n",
    "        polished = llm_pipeline(prompt)[0]['generated_text'].split(\"A:\")[-1].strip()\n",
    "        return f\"{polished}<br><br>{suggest_related(query)}\", source if source else \"Custom Knowledge Base\"\n",
    "\n",
    "# === UI Code ===\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"M^6 AI Tutor\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style='text-align:center; background:linear-gradient(90deg,#fff1c1,#c1e7ff,#e1ffc1); border-radius:15px; padding:10px 0; margin-bottom:8px; color:#000;'>\n",
    "      <h1 style='color:#000;'>üß† M^6 AI Tutor</h1>\n",
    "      <h3 style='color:#000;'>Switch between <span style='color:#0a88a4'>Tutor Mode</span> and <span style='color:#0a88a4'>Game Mode</span> for interactive learning!</h3>\n",
    "      <p style='color:#111; font-weight:700; font-size:1.05em;'>\n",
    "        <b>Learn, Play, and Benchmark real data science and GPU acceleration!</b>\n",
    "      </p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        # ---- Tutor Mode Tab ----\n",
    "        with gr.Tab(\"Tutor Mode\"):\n",
    "            tutor_query = gr.Textbox(label=\"Ask about Data Science or GPU topics\", placeholder=\"e.g., What is RAPIDS?\")\n",
    "            tutor_answer_box = gr.Markdown(label=\"AI Tutor's Answer\", value=\"\")\n",
    "            tutor_source_box = gr.Markdown(label=\"Source of Answer\", value=\"\")\n",
    "            tutor_btn = gr.Button(\"üöÄ Get Tutor Help\")\n",
    "            def tutor_handler(q):\n",
    "                answer, source = smart_tutor_answer(q)\n",
    "                return answer, f\"**Source:** {source}\"\n",
    "            tutor_btn.click(fn=tutor_handler, inputs=tutor_query, outputs=[tutor_answer_box, tutor_source_box])\n",
    "\n",
    "        # ---- Game Mode Tab ----\n",
    "        with gr.Tab(\"Game Mode\"):\n",
    "            from types import SimpleNamespace\n",
    "            user_state = SimpleNamespace(points=0, level=1)\n",
    "\n",
    "            def update_level():\n",
    "                \"\"\"Points/level and progress bar.\"\"\"\n",
    "                user_state.level = min(10, 1 + user_state.points // 10)\n",
    "                bar = f\"\"\"<b>‚≠ê {user_state.points} points ‚Äî Level {user_state.level}</b><br>\n",
    "                <div style='background:#eee;border-radius:8px;width:100%;height:18px;'>\n",
    "                <div style='background:linear-gradient(90deg,#fceabb,#f8b500);height:18px;width:{(user_state.points % 10)*10}%;border-radius:8px;'></div>\n",
    "                </div>\"\"\"\n",
    "                return bar\n",
    "\n",
    "            progress = gr.HTML(update_level())\n",
    "\n",
    "            # --- Flashcards ---\n",
    "            FLASHCARDS = [\n",
    "                {\"front\": \"What is cuDF?\", \"back\": \"A RAPIDS GPU DataFrame library, similar to pandas but on GPU.\"},\n",
    "                {\"front\": \"What is a CUDA core?\", \"back\": \"A processing unit on an NVIDIA GPU for parallel computing.\"},\n",
    "                {\"front\": \"Why use RAPIDS?\", \"back\": \"RAPIDS enables GPU-accelerated DataFrame operations similar to pandas.\"},\n",
    "                {\"front\": \"How do you create a DataFrame in pandas?\", \"back\": \"import pandas as pd; df = pd.DataFrame(...)\"}\n",
    "            ]\n",
    "            def get_flashcard(idx, show):\n",
    "                card = FLASHCARDS[idx % len(FLASHCARDS)]\n",
    "                front = f\"üÉè Q: {card['front']}\"\n",
    "                back = f\"A: {card['back']}\" if show else \"Click 'Show Answer'\"\n",
    "                return front, back, f\"Card {idx+1}/{len(FLASHCARDS)}\", idx % len(FLASHCARDS), show\n",
    "\n",
    "            with gr.Tabs():\n",
    "                # --- Flashcards Tab ---\n",
    "                with gr.Tab(\"üÉè Flashcards\"):\n",
    "                    flash_idx = gr.State(0)\n",
    "                    show_answer = gr.State(False)\n",
    "                    card_picker = gr.Dropdown(choices=[f\"Card {i+1}\" for i in range(len(FLASHCARDS))], label=\"Choose a Flashcard\", value=\"Card 1\")\n",
    "                    card_front = gr.Markdown()\n",
    "                    card_back = gr.Markdown()\n",
    "                    card_count = gr.Markdown()\n",
    "                    flip_btn = gr.Button(\"üîÑ Flip Card\")\n",
    "                    reset_btn = gr.Button(\"üîÅ Reset\")\n",
    "\n",
    "                    # Change card by dropdown\n",
    "                    def pick_card(label):\n",
    "                        idx = int(label.split()[-1]) - 1\n",
    "                        return get_flashcard(idx, False)\n",
    "                    # Flip card to show/hide answer\n",
    "                    def flip_card(idx, show):\n",
    "                        return get_flashcard(idx, not show)\n",
    "                    # Reset to first card\n",
    "                    def reset_flashcards():\n",
    "                        return get_flashcard(0, False)\n",
    "\n",
    "                    card_picker.change(pick_card, inputs=card_picker, outputs=[card_front, card_back, card_count, flash_idx, show_answer])\n",
    "                    flip_btn.click(flip_card, inputs=[flash_idx, show_answer], outputs=[card_front, card_back, card_count, flash_idx, show_answer])\n",
    "                    reset_btn.click(reset_flashcards, outputs=[card_front, card_back, card_count, flash_idx, show_answer])\n",
    "\n",
    "                # --- Quiz Tab ---\n",
    "                with gr.Tab(\"‚ùì Quiz\"):\n",
    "                    quiz_pool = [\n",
    "                        (\"Which library is used for GPU DataFrame processing?\", [\"pandas\", \"numpy\", \"RAPIDS\", \"sklearn\"], 2),\n",
    "                        (\"Who develops CUDA?\", [\"AMD\", \"NVIDIA\", \"Intel\", \"Google\"], 1)\n",
    "                    ]\n",
    "                    quiz_state = gr.State(quiz_pool)\n",
    "                    quiz_question_1 = gr.Markdown(value=f\"**Q1:** {quiz_pool[0][0]}\")\n",
    "                    quiz_radio_1 = gr.Radio(choices=quiz_pool[0][1], label=\"\")\n",
    "                    quiz_question_2 = gr.Markdown(value=f\"**Q2:** {quiz_pool[1][0]}\")\n",
    "                    quiz_radio_2 = gr.Radio(choices=quiz_pool[1][1], label=\"\")\n",
    "                    quiz_btn = gr.Button(\"Submit Quiz\")\n",
    "                    quiz_out = gr.Markdown()\n",
    "\n",
    "                    def evaluate_quiz(ans1, ans2, pool):\n",
    "                        correct = 0\n",
    "                        if ans1 == pool[0][1][pool[0][2]]:\n",
    "                            correct += 1\n",
    "                        if ans2 == pool[1][1][pool[1][2]]:\n",
    "                            correct += 1\n",
    "                        user_state.points += correct * 3\n",
    "                        result = f\"‚úÖ {correct}/2 Correct. +{correct*3} pts\"\n",
    "                        return result, update_level()\n",
    "                    quiz_btn.click(fn=evaluate_quiz, inputs=[quiz_radio_1, quiz_radio_2, quiz_state], outputs=[quiz_out, progress])\n",
    "\n",
    "                # --- Coding Puzzle Tab ---\n",
    "                with gr.Tab(\"üíª Coding Puzzle\"):\n",
    "                    gr.Markdown(\"<b>Benchmark CPU vs GPU</b><br>Check your cuDF/cuML code accuracy and visualize speedup.\")\n",
    "                    puzzle = {\n",
    "                        \"desc\": \"Convert Pandas CPU code to cuDF GPU\",\n",
    "                        \"cpu\": \"import pandas as pd\\n\"\n",
    "                               \"df = pd.DataFrame({'a': [1,2,3]})\\n\"\n",
    "                               \"print(df.sum())\",\n",
    "                        \"gpu\": \"import cudf\\n\"\n",
    "                               \"df = cudf.DataFrame({'a': [1,2,3]})\\n\"\n",
    "                               \"print(df.sum())\"\n",
    "                    }\n",
    "                    cpu_code = gr.Code(label=\"CPU Code\", value=puzzle['cpu'], interactive=False)\n",
    "                    user_code = gr.Code(label=\"Your GPU Code\", language=\"python\")\n",
    "                    check_btn = gr.Button(\"Check Solution üö¶\")\n",
    "                    bench_btn = gr.Button(\"Run Benchmark üöÄ\")\n",
    "                    feedback = gr.Markdown()\n",
    "                    bench_out = gr.Markdown()\n",
    "\n",
    "                    def check_gpu_code(code):\n",
    "                        match = difflib.SequenceMatcher(None, code.strip(), puzzle['gpu'].strip()).ratio()\n",
    "                        if match > 0.7:\n",
    "                            user_state.points += 5\n",
    "                            return \"üéâ Looks good! +5 points\", update_level()\n",
    "                        else:\n",
    "                            return \"‚ùå Not quite. Try matching the structure of cuDF.\", update_level()\n",
    "                    def run_benchmark(code):\n",
    "                        match = difflib.SequenceMatcher(None, code.strip(), puzzle['gpu'].strip()).ratio()\n",
    "                        if match > 0.7:\n",
    "                            cpu_time = 1.5\n",
    "                            gpu_time = 0.2\n",
    "                            speedup = round(cpu_time / gpu_time, 1)\n",
    "                            return f\"‚è±Ô∏è CPU time: {cpu_time}s<br>‚ö° GPU time: {gpu_time}s<br>üöÄ Speedup: {speedup}x\"\n",
    "                        else:\n",
    "                            return \"‚ùå Code not GPU-convertible. Fix and try again.\"\n",
    "                    check_btn.click(check_gpu_code, [user_code], [feedback, progress])\n",
    "                    bench_btn.click(run_benchmark, [user_code], [bench_out])\n",
    "\n",
    "app.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
